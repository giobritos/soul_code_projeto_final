{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giobritos/soul_code_projeto_final/blob/main/Custom_Template_GCS_para_BigQuery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando um custom template do Cloud Dataflow para enviar aquivos da GCS para o BigQuery"
      ],
      "metadata": {
        "id": "mezUQFlsEJsD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalações (reiniciar a cada instalação)"
      ],
      "metadata": {
        "id": "oF0sEMKEj6mX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKp9RguBah2H"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install apache_beam[interactive]"
      ],
      "metadata": {
        "id": "C_P21ziEYayw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install apache_beam[gcp]"
      ],
      "metadata": {
        "id": "WIEtYxnjYcGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conexão a GCP"
      ],
      "metadata": {
        "id": "GnGFYTH9kFwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Configuração da chave de segurança\n",
        "serviceAccount = '/content/projeto-final-373521-25961e56ca37.json'\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = serviceAccount"
      ],
      "metadata": {
        "id": "NCoGh-KSZ0wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando o template"
      ],
      "metadata": {
        "id": "5feaq2vK8Wc-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoE2rXOE57bO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "f41acbd2-5e52-4693-f628-4f0066251378"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_1848/3711472189.py:53: BeamDeprecationWarning: BigQuerySink is deprecated since 2.11.0. Use WriteToBigQuery instead.\n",
            "  beam.io.BigQuerySink(\n",
            "/usr/local/lib/python3.8/dist-packages/apache_beam/io/gcp/bigquery.py:1281: UserWarning: Native sinks no longer implemented; falling back to standard Beam sink.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/apache_beam/io/gcp/bigquery.py:1988: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
            "  self.table_reference.projectId = pcoll.pipeline.options.view_as(\n",
            "/usr/local/lib/python3.8/dist-packages/apache_beam/io/gcp/bigquery_file_loads.py:1142: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
            "  self.project = self.project or p.options.view_as(GoogleCloudOptions).project\n"
          ]
        }
      ],
      "source": [
        "# Arquivo base:\n",
        "# ETL Processing on Google Cloud Using Dataflow and BigQuery\n",
        "# https://www.cloudskillsboost.google/focuses/3460?parent=catalog\n",
        "\n",
        "import re\n",
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "# Classe com os passos para transformar um arquivo CSV \n",
        "# em um formato aceito pelo BigQuery\n",
        "class DataIngestion:\n",
        "    # Método que pega uma linha de dados separados por ',' \n",
        "    # e transforma em dicionário para ser carregado no BigQuery\n",
        "    def parse_method(self, string_input):\n",
        "        # Separando a linha por ',' e removendo '\"' , '\\n' e '\\r'\n",
        "        values = re.split(\",\", re.sub('\\r\\n','',re.sub('\"','',string_input)))\n",
        "        # Criando um dicionário com os dados\n",
        "        row = dict(\n",
        "            zip(('cidade', 'tipo_crime', 'distrito_zona', 'data_ocorrencia',\n",
        "                 'periodo_ocorrencia', 'faixa_etaria_suspeito', \n",
        "                 'cor_pele_suspeito', 'sexo_suspeito', 'faixa_etaria_vitima', \n",
        "                 'cor_pele_vitima', 'sexo_vitima', 'latitude_longitude', \n",
        "                 'desc_foro', 'n_vitimas', 'tipo_pessoa', 'dt_nasc_pessoa', \n",
        "                 'idade_pessoa', 'profissao', 'tipo_local_fato', \n",
        "                 'logradouro', 'n_logradouro', 'corporacao_policial', \n",
        "                 'situacao_policial', 'dp_num_nome', 'bairro'),\n",
        "                values))\n",
        "        return row\n",
        "\n",
        "# Função para criar o custom template\n",
        "def run(argv=None):\n",
        "\n",
        "  # Instanciando a classe DataIngestion\n",
        "  data_ingestion = DataIngestion()\n",
        "\n",
        "  # Argumentos da pipeline\n",
        "  pipeline_options_dict = {\n",
        "    'runner':'DataflowRunner',\n",
        "    'project':'projeto-final-373521',\n",
        "    'staging_location':'gs://pipeline-apachebeam/staging',\n",
        "    'temp_location':'gs://pipeline-apachebeam/temp',\n",
        "    'template_location':'gs://pipeline-apachebeam/models/nyspGCStoBQ',\n",
        "    'region':'southamerica-east1',\n",
        "    'input':'gs://projeto-final-agsw/tratados/ny_sp_tratado.csv',\n",
        "    'output':'dataset_projeto_final_pipeline.ny_sp_tratado_base'\n",
        "  }\n",
        "\n",
        "  pipeline_options = PipelineOptions.from_dictionary(pipeline_options_dict)\n",
        "\n",
        "  # Iniciando a pipeline\n",
        "  p = beam.Pipeline(options=pipeline_options)\n",
        "\n",
        "  (p\n",
        "    # Lendo o arquivo, pulando a primeira linha \n",
        "    # que tem as informações das colunas\n",
        "    | 'Lendo o CSV' >> beam.io.ReadFromText(pipeline_options_dict.get('input'),\\\n",
        "                                            skip_header_lines=1)\n",
        "    # Usando o método parse_method da classe DataIngestion \n",
        "    # para preparar os dados\n",
        "    | 'Preparando os dados' >> beam.Map(lambda s:data_ingestion.parse_method(s))\n",
        "    # Escrevendo no BigQuery\n",
        "    | 'Escrevendo no BigQuery' >> beam.io.Write(\n",
        "          beam.io.BigQuerySink(\n",
        "          # Nome da tabela\n",
        "          pipeline_options_dict.get('output'),\n",
        "          # Esquema da tabela\n",
        "          schema=\"cidade:STRING, tipo_crime:STRING, distrito_zona:STRING,\"\\\n",
        "                 \"data_ocorrencia:TIMESTAMP, periodo_ocorrencia:STRING,\"\\ \n",
        "                 \"faixa_etaria_suspeito:STRING, cor_pele_suspeito:STRING,\"\\\n",
        "                 \"sexo_suspeito:STRING, faixa_etaria_vitima:STRING,\"\\ \n",
        "                 \"cor_pele_vitima:STRING, sexo_vitima:STRING,\"\\ \n",
        "                 \"latitude_longitude:STRING, desc_foro:STRING,\"\\\n",
        "                 \"n_vitimas:STRING, tipo_pessoa:STRING,\"\\\n",
        "                 \"dt_nasc_pessoa:STRING, idade_pessoa:STRING,\"\\\n",
        "                 \"profissao:STRING, tipo_local_fato:STRING, logradouro:STRING,\"\\ \n",
        "                 \"n_logradouro:STRING, corporacao_policial:STRING, \"\\\n",
        "                 \"situacao_policial:STRING, dp_num_nome:STRING, bairro:STRING\",\n",
        "          # Cria a tabela se ela não existir\n",
        "          create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
        "          # Deleta os dados da tabela existente antes de escrever\n",
        "          write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE))\n",
        "  )\n",
        "  p.run().wait_until_finish()\n",
        "run()"
      ]
    }
  ]
}